{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>removed_punc</th>\n",
       "      <th>tokens</th>\n",
       "      <th>filtered_tokens</th>\n",
       "      <th>clean_tokens</th>\n",
       "      <th>lemma_words</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "      <td>House Dem Aide We Didn’t Even See Comey’s Lett...</td>\n",
       "      <td>['house', 'dem', 'aide', 'we', 'didn’t', 'even...</td>\n",
       "      <td>['house', 'aide', 'didn’t', 'even', 'comey’s',...</td>\n",
       "      <td>['house', 'aide', 'didn’t', 'even', 'comey’s',...</td>\n",
       "      <td>['house', 'aide', 'didn’t', 'even', 'comey’s',...</td>\n",
       "      <td>house aide didn’t even comey’s letter jason ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>['ever', 'get', 'the', 'feeling', 'your', 'lif...</td>\n",
       "      <td>['ever', 'feeling', 'your', 'life', 'circles',...</td>\n",
       "      <td>['ever', 'feeling', 'life', 'circles', 'rounda...</td>\n",
       "      <td>['ever', 'feeling', 'life', 'circle', 'roundab...</td>\n",
       "      <td>ever feeling life circle roundabout rather hea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29 2...</td>\n",
       "      <td>['why', 'the', 'truth', 'might', 'get', 'you',...</td>\n",
       "      <td>['truth', 'might', 'fired', 'october', '2016',...</td>\n",
       "      <td>['truth', 'might', 'fired', 'october', '2016',...</td>\n",
       "      <td>['truth', 'might', 'fired', 'october', '2016',...</td>\n",
       "      <td>truth might fired october 2016 tension intelli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>['videos', '15', 'civilians', 'killed', 'in', ...</td>\n",
       "      <td>['videos', 'civilians', 'killed', 'single', 'a...</td>\n",
       "      <td>['videos', 'civilians', 'killed', 'single', 'a...</td>\n",
       "      <td>['video', 'civilian', 'killed', 'single', 'air...</td>\n",
       "      <td>video civilian killed single airstrike identif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>1</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>['print', 'an', 'iranian', 'woman', 'has', 'be...</td>\n",
       "      <td>['print', 'iranian', 'woman', 'been', 'sentenc...</td>\n",
       "      <td>['print', 'iranian', 'woman', 'sentenced', 'ye...</td>\n",
       "      <td>['print', 'iranian', 'woman', 'sentenced', 'ye...</td>\n",
       "      <td>print iranian woman sentenced year prison iran...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  ...                                         clean_text\n",
       "0  House Dem Aide: We Didn’t Even See Comey’s Let...  ...  house aide didn’t even comey’s letter jason ch...\n",
       "1  FLYNN: Hillary Clinton, Big Woman on Campus - ...  ...  ever feeling life circle roundabout rather hea...\n",
       "2                  Why the Truth Might Get You Fired  ...  truth might fired october 2016 tension intelli...\n",
       "3  15 Civilians Killed In Single US Airstrike Hav...  ...  video civilian killed single airstrike identif...\n",
       "4  Iranian woman jailed for fictional unpublished...  ...  print iranian woman sentenced year prison iran...\n",
       "\n",
       "[5 rows x 10 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/preprocessed_train.csv', encoding='utf-8')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        house aide didn’t even comey’s letter jason ch...\n",
       "2        truth might fired october 2016 tension intelli...\n",
       "3        video civilian killed single airstrike identif...\n",
       "4        print iranian woman sentenced year prison iran...\n",
       "10       mystery surrounding third reich nazi germany s...\n",
       "                               ...                        \n",
       "18229    google pinterest digg linkedin reddit stumbleu...\n",
       "18232    lawyer kept hillary campaign chief jail hillar...\n",
       "18234    share although vandal thought “cool” destroy d...\n",
       "18239    nato russia hold parallel exercise balkan 1102...\n",
       "18240    david swanson author activist journalist radio...\n",
       "Name: clean_text, Length: 7880, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = df.loc[df['label'] == 1, 'clean_text']\n",
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Here we should be reading from the FAKE NEWS\n",
    "texts = data.tolist()\n",
    "texts = [text for text in texts if not (isinstance(text, float) and np.isnan(text))]\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "total_words = len(tokenizer.word_index) + 1  # Vocabulary size\n",
    "\n",
    "input_sequences = []\n",
    "for line in texts:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "max_sequence_length = max([len(seq) for seq in input_sequences])\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='pre')\n",
    "\n",
    "X = input_sequences[:, :-1]\n",
    "y = input_sequences[:, -1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, input_length=max_sequence_length - 1))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Train your model using X and y\n",
    "# You might want to one-hot encode y for categorical crossentropy\n",
    "\n",
    "# Generate text using the trained model\n",
    "seed_text = \"Your starting text here\"\n",
    "next_words = 100\n",
    "\n",
    "for _ in range(next_words):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_length - 1, padding='pre')\n",
    "    predicted = model.predict(token_list, verbose=0)\n",
    "    predicted_word = tokenizer.index_word[np.argmax(predicted)]\n",
    "    seed_text += \" \" + predicted_word\n",
    "\n",
    "print(seed_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator with Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\programas\\programas\\github repos\\nlp\\nlp\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Fake News:  Your starting text here sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence 3 sentence sentence 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 1 1 1 1 1 sentence sentence sentence 3 3 3 3 3 3 3 3 3 3 3 3 3 3 sentence 1 1 1 1 1 1 1 sentence sentence sentence sentence sentence 1 1 1 sentence sentence 1 1 1 1 1 1 1 1 1 1 1 sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence 3 sentence sentence sentence sentence sentence sentence 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 sentence sentence sentence 3 3 3 3 3 3 sentence sentence sentence 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence sentence 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config, AdamW\n",
    "\n",
    "# Here we should be reading from the FAKE NEWS\n",
    "text_data = [\n",
    "    \"Fake news sentence 1\",\n",
    "    \"Fake news sentence 2\",\n",
    "    \"Fake news sentence 3\",\n",
    "    # Add more fake news sentences as needed\n",
    "]\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenized_data = tokenizer(text_data, truncation=True, max_length=512, return_tensors='pt')\n",
    "\n",
    "# Model Training\n",
    "config = GPT2Config.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel(config)\n",
    "train_dataset = torch.utils.data.TensorDataset(tokenized_data['input_ids'], tokenized_data['attention_mask'])\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask = batch\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "# Generate text using the trained model\n",
    "seed_text = \"give a title for a news article\"\n",
    "input_ids = tokenizer.encode(seed_text, return_tensors='pt')\n",
    "output = model.generate(input_ids, max_length=200, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated Fake News: \", generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
