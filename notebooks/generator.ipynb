{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>removed_punc</th>\n",
       "      <th>tokens</th>\n",
       "      <th>filtered_tokens</th>\n",
       "      <th>clean_tokens</th>\n",
       "      <th>lemma_words</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "      <td>House Dem Aide We Didn’t Even See Comey’s Lett...</td>\n",
       "      <td>['house', 'dem', 'aide', 'we', 'didn’t', 'even...</td>\n",
       "      <td>['house', 'aide', 'didn’t', 'even', 'comey’s',...</td>\n",
       "      <td>['house', 'aide', 'didn’t', 'even', 'comey’s',...</td>\n",
       "      <td>['house', 'aide', 'didn’t', 'even', 'comey’s',...</td>\n",
       "      <td>house aide didn’t even comey’s letter jason ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>['ever', 'get', 'the', 'feeling', 'your', 'lif...</td>\n",
       "      <td>['ever', 'feeling', 'your', 'life', 'circles',...</td>\n",
       "      <td>['ever', 'feeling', 'life', 'circles', 'rounda...</td>\n",
       "      <td>['ever', 'feeling', 'life', 'circle', 'roundab...</td>\n",
       "      <td>ever feeling life circle roundabout rather hea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29 2...</td>\n",
       "      <td>['why', 'the', 'truth', 'might', 'get', 'you',...</td>\n",
       "      <td>['truth', 'might', 'fired', 'october', '2016',...</td>\n",
       "      <td>['truth', 'might', 'fired', 'october', '2016',...</td>\n",
       "      <td>['truth', 'might', 'fired', 'october', '2016',...</td>\n",
       "      <td>truth might fired october 2016 tension intelli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>['videos', '15', 'civilians', 'killed', 'in', ...</td>\n",
       "      <td>['videos', 'civilians', 'killed', 'single', 'a...</td>\n",
       "      <td>['videos', 'civilians', 'killed', 'single', 'a...</td>\n",
       "      <td>['video', 'civilian', 'killed', 'single', 'air...</td>\n",
       "      <td>video civilian killed single airstrike identif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>1</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>['print', 'an', 'iranian', 'woman', 'has', 'be...</td>\n",
       "      <td>['print', 'iranian', 'woman', 'been', 'sentenc...</td>\n",
       "      <td>['print', 'iranian', 'woman', 'sentenced', 'ye...</td>\n",
       "      <td>['print', 'iranian', 'woman', 'sentenced', 'ye...</td>\n",
       "      <td>print iranian woman sentenced year prison iran...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title              author  \\\n",
       "0  House Dem Aide: We Didn’t Even See Comey’s Let...       Darrell Lucus   \n",
       "1  FLYNN: Hillary Clinton, Big Woman on Campus - ...     Daniel J. Flynn   \n",
       "2                  Why the Truth Might Get You Fired  Consortiumnews.com   \n",
       "3  15 Civilians Killed In Single US Airstrike Hav...     Jessica Purkiss   \n",
       "4  Iranian woman jailed for fictional unpublished...      Howard Portnoy   \n",
       "\n",
       "                                                text  label  \\\n",
       "0  House Dem Aide: We Didn’t Even See Comey’s Let...      1   \n",
       "1  Ever get the feeling your life circles the rou...      0   \n",
       "2  Why the Truth Might Get You Fired October 29, ...      1   \n",
       "3  Videos 15 Civilians Killed In Single US Airstr...      1   \n",
       "4  Print \\nAn Iranian woman has been sentenced to...      1   \n",
       "\n",
       "                                        removed_punc  \\\n",
       "0  House Dem Aide We Didn’t Even See Comey’s Lett...   \n",
       "1  Ever get the feeling your life circles the rou...   \n",
       "2  Why the Truth Might Get You Fired October 29 2...   \n",
       "3  Videos 15 Civilians Killed In Single US Airstr...   \n",
       "4  Print \\nAn Iranian woman has been sentenced to...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  ['house', 'dem', 'aide', 'we', 'didn’t', 'even...   \n",
       "1  ['ever', 'get', 'the', 'feeling', 'your', 'lif...   \n",
       "2  ['why', 'the', 'truth', 'might', 'get', 'you',...   \n",
       "3  ['videos', '15', 'civilians', 'killed', 'in', ...   \n",
       "4  ['print', 'an', 'iranian', 'woman', 'has', 'be...   \n",
       "\n",
       "                                     filtered_tokens  \\\n",
       "0  ['house', 'aide', 'didn’t', 'even', 'comey’s',...   \n",
       "1  ['ever', 'feeling', 'your', 'life', 'circles',...   \n",
       "2  ['truth', 'might', 'fired', 'october', '2016',...   \n",
       "3  ['videos', 'civilians', 'killed', 'single', 'a...   \n",
       "4  ['print', 'iranian', 'woman', 'been', 'sentenc...   \n",
       "\n",
       "                                        clean_tokens  \\\n",
       "0  ['house', 'aide', 'didn’t', 'even', 'comey’s',...   \n",
       "1  ['ever', 'feeling', 'life', 'circles', 'rounda...   \n",
       "2  ['truth', 'might', 'fired', 'october', '2016',...   \n",
       "3  ['videos', 'civilians', 'killed', 'single', 'a...   \n",
       "4  ['print', 'iranian', 'woman', 'sentenced', 'ye...   \n",
       "\n",
       "                                         lemma_words  \\\n",
       "0  ['house', 'aide', 'didn’t', 'even', 'comey’s',...   \n",
       "1  ['ever', 'feeling', 'life', 'circle', 'roundab...   \n",
       "2  ['truth', 'might', 'fired', 'october', '2016',...   \n",
       "3  ['video', 'civilian', 'killed', 'single', 'air...   \n",
       "4  ['print', 'iranian', 'woman', 'sentenced', 'ye...   \n",
       "\n",
       "                                          clean_text  \n",
       "0  house aide didn’t even comey’s letter jason ch...  \n",
       "1  ever feeling life circle roundabout rather hea...  \n",
       "2  truth might fired october 2016 tension intelli...  \n",
       "3  video civilian killed single airstrike identif...  \n",
       "4  print iranian woman sentenced year prison iran...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/preprocessed_train.csv', encoding='utf-8')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.loc[df['label'] == 1, 'clean_text'][:50]\n",
    "test = df.loc[df['label'] == 1, 'clean_text'][6000:]\n",
    "text_list = data.tolist()\n",
    "test = test.tolist()\n",
    "text_list= [text for text in text_list if not (isinstance(text, float) and np.isnan(text))]\n",
    "test= [text for text in test if not (isinstance(text, float) and np.isnan(text))]\n",
    "#del text_list[7406] #este indice es un string con palabras en ruso\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator with BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "\n",
    "# Load the pre-trained BERT model and tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForMaskedLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\programas\\programas\\github repos\\nlp\\nlp\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "# Tokenize your dataset\n",
    "max_length = 512\n",
    "tokenized_dataset = tokenizer(text_list, truncation=True, max_length=max_length, return_tensors='pt')\n",
    "\n",
    "# Define the device\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# Get the input IDs and attention mask tensors from the tokenized dataset\n",
    "input_ids = tokenized_dataset['input_ids']\n",
    "attention_mask = tokenized_dataset['attention_mask']\n",
    "\n",
    "# Create a TensorDataset object from the input_ids and attention_mask tensors\n",
    "dataset = torch.utils.data.TensorDataset(input_ids, attention_mask)\n",
    "#input_ids = input_ids.to(device)\n",
    "#attention_mask = attention_mask.to(device)\n",
    "\n",
    "# Define the optimizer and learning rate\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 1.5594005584716797\n",
      "Epoch 2 loss: 0.35310813784599304\n",
      "Epoch 3 loss: 0.19861313700675964\n"
     ]
    }
   ],
   "source": [
    "# Define your training loop\n",
    "num_epochs = 3\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_loader:\n",
    "\n",
    "        # Get the input and target sequences\n",
    "        input_ids = batch[0]\n",
    "        attention_mask = batch[1]\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = outputs.logits.view(-1, outputs.logits.shape[-1]).float().to(device)\n",
    "        labels = input_ids.view(-1).to(device)\n",
    "        loss = torch.nn.functional.cross_entropy(loss, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    print(f'Epoch {epoch+1} loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "argentina imposes quentin garrett ’ lethal suspension revelationlay niall\n"
     ]
    }
   ],
   "source": [
    "# Define the prompt\n",
    "prompt = \"Argentina imposes \"\n",
    "\n",
    "# Tokenize the prompt\n",
    "input_ids = tokenizer.encode(prompt, add_special_tokens=False, return_tensors='pt')\n",
    "\n",
    "# Generate text\n",
    "max_length = 12\n",
    "output = model.generate(input_ids=input_ids, max_length=max_length, repetition_penalty=1.6)\n",
    "\n",
    "# Decode the generated text\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "donald trumprockieu ’ ” conclusion quentin suspensionguelay niall patrice notwithstanding “ rwanda thanksgiving sava brynweelmanibly genocide countlessbh din kellan civilian resultith nialliling\n"
     ]
    }
   ],
   "source": [
    "# Define the prompt\n",
    "prompt = \"Donald Trump\"\n",
    "# Generate text\n",
    "max_length = 6\n",
    "\n",
    "for _ in range(10):\n",
    "    # Tokenize the generated text\n",
    "    input_ids = tokenizer.encode(prompt, add_special_tokens=False, return_tensors='pt')\n",
    "\n",
    "    # Generate text\n",
    "    output = model.generate(input_ids=input_ids, max_length=max_length, repetition_penalty=1.5)\n",
    "\n",
    "    # Decode the generated text\n",
    "    prompt = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    max_length += 3 \n",
    "\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator with Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = text_list[:6000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenization of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\programas\\programas\\github repos\\nlp\\nlp\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AdamW, GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenized_dataset = tokenizer.encode(text_data, add_special_tokens=True, max_length=1024, truncation=True)\n",
    "\n",
    "# Convert the tokenized dataset to tensors\n",
    "input_ids = torch.tensor(tokenized_dataset[:-1]).unsqueeze(0)\n",
    "labels = torch.tensor(tokenized_dataset[1:]).unsqueeze(0)\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(input_ids, labels)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=10, shuffle=True)\n",
    "\n",
    "# Load the pre-trained GPT-2 model\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Fine-tune the model\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(5):\n",
    "    for batch in train_loader:\n",
    "\n",
    "        # Get the input and target sequences\n",
    "        input_ids = batch[0]\n",
    "        target_ids = batch[1]\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, labels=target_ids)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si se juega con los parametros ingresados en el metodo generate es posible cambiar la estructura de la generación de noticias. Revisar la documentación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Fake News:  ['Argentina grows its own food.\\nThe country\\'s agriculture minister, Josef Kostas said that the government would not be able to provide a solution for farmers in this area because of lack and need from time-to: \"We are going through an economic crisis.\" He added he was concerned about how it could happen with such large scale agricultural production as there is no way possible without using any kind or all methods at present,\" Mr.Kosovo told reporters on Wednesday (Nov 17).']\n"
     ]
    }
   ],
   "source": [
    "seed_text = \"Argentina grows\"\n",
    "input_ids = tokenizer.encode(seed_text, return_tensors='pt')\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    max_length=300,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    repetition_penalty=2.0,\n",
    ")\n",
    "# Decode the generated sequences\n",
    "generated_text = []\n",
    "for sequence in output:\n",
    "    sequence = sequence.tolist()\n",
    "    text = tokenizer.decode(sequence, skip_special_tokens=True)\n",
    "    generated_text.append(text)\n",
    "\n",
    "print(\"Generated Fake News: \", generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = text_list[6000:]\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_tokens = tokenizer.encode(test, add_special_tokens=True, max_length=512, truncation=True)\n",
    "# Convert the tokenized dataset to tensors\n",
    "input_ids = torch.tensor(test_set_tokens[:-1]).unsqueeze(0)\n",
    "labels = torch.tensor(test_set_tokens[1:]).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity score:  tensor(818.2751, grad_fn=<ExpBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = model(input_ids = input_ids, labels = labels).loss\n",
    "ppl = torch.exp(loss)\n",
    "print(\"Perplexity score: \", ppl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "perplexity gpt2: 1.4392; perplexity bert: 818"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
